{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "%pylab is deprecated, use %matplotlib inline and import the required libraries.\n",
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "import time\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Keras 92% 16s For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-03 22:41:05.573901: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n",
      "2022-05-03 22:41:05.694178: W tensorflow/core/framework/cpu_allocator_impl.cc:82] Allocation of 188160000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1875/1875 [==============================] - 4s 2ms/step - loss: 0.3829 - accuracy: 0.8950\n",
      "313/313 [==============================] - 1s 2ms/step - loss: 0.2848 - accuracy: 0.9202\n",
      "test loss, test acc: [0.2847595810890198, 0.920199990272522]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D  \n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
    "Y_train = tf.keras.utils.to_categorical(Y_train)\n",
    "Y_test = tf.keras.utils.to_categorical(Y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(28, name='dense_in', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(10, name='dense_last', activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=1, verbose=1)\n",
    "results = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "- Try Cross Entropy Loss\n",
    "- Find why algo so slow\n",
    "- ***Working*** - Batching\n",
    "- Dropout\n",
    "- Max Pooling\n",
    "- Fix code redundancy between batched and non batched\n",
    "- ***Working*** - Data Augmentation\n",
    "- Better learning decay algo\n",
    "\n",
    "Data Augmentation - Mostly it seems to add time if your goal is just 96% or so. But to get over 97% it might help. I'll try to run a test to confirm this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "def add_noise(x):\n",
    "    amt = 10\n",
    "    return (x + amt*np.random.rand(x.shape[0], x.shape[1]))/(1+(amt/255))\n",
    "\n",
    "def rot(x):\n",
    "    ang = np.random.rand()*30-15 # Degrees\n",
    "    h, w = x.shape # Numpy puts out the image axes in the wrong order\n",
    "    cx = w / 2\n",
    "    cy = h / 2\n",
    "    theta = ang * 3.14 / 180\n",
    "    ang = -theta\n",
    "    rotmat = np.array([[np.cos(ang), -np.sin(ang)],\n",
    "                    [np.sin(ang), np.cos(ang)]])\n",
    "\n",
    "    new_im = np.zeros((h, w))\n",
    "    for i in range(w):\n",
    "        for j in range(h):\n",
    "            newx = i - cx\n",
    "            newy = (h-j) - cy\n",
    "            vec = np.matmul(rotmat, np.array([[newx],[newy]]))\n",
    "            oldx = round(vec[0][0] + cx)\n",
    "            oldy = round(cy-vec[1][0])\n",
    "            if oldx >= 0 and oldx < w and oldy >= 0 and oldy < h:\n",
    "                new_im[j][i] = x[oldy][oldx]\n",
    "    return new_im\n",
    "\n",
    "def shift_hor(x):\n",
    "    if np.random.rand() >= 0.999:\n",
    "        for i in range(x.shape[1]-1): # Left\n",
    "            x[:,i] = x[:,i+1]\n",
    "    else:\n",
    "        for i in range(x.shape[1]): # Right\n",
    "            i = x.shape[0] - i\n",
    "            if i <27:\n",
    "                x[:,i+1] = x[:,i]\n",
    "    return x\n",
    "\n",
    "def shift_vert(x):\n",
    "    if np.random.rand() >= 0.5:\n",
    "        for i in range(x.shape[0]-1): # Up\n",
    "            x[i] = x[i+1]\n",
    "    else:\n",
    "        for i in range(x.shape[0]): # Down\n",
    "            i = x.shape[0] - i\n",
    "            if i <27:\n",
    "                x[i] = x[i-1]\n",
    "    return x\n",
    "\n",
    "def smooth_blur(x):\n",
    "    size = 3\n",
    "    w, h = x.shape\n",
    "    kernel = np.ones((size,size)) # Smooth Kernel\n",
    "    for a in range(w-size+1):\n",
    "        for b in range(h-size+1):\n",
    "            x[a][b] = (x[a][b] + 0.25*np.sum( x[a:a+size, b:b+size]*kernel ) / (size**2) ) / 1.25\n",
    "    return x\n",
    "\n",
    "def data_aug(x):\n",
    "    '''\n",
    "    Calls functions to augment data\n",
    "    '''\n",
    "    if np.random.rand() >= 100: # Adds more compute time than it removes\n",
    "        x = smooth_blur(x)\n",
    "    if np.random.rand() >= 0.5:\n",
    "        x = shift_vert(x)\n",
    "    if np.random.rand() >= 0.5:\n",
    "        x = shift_hor(x)\n",
    "    if np.random.rand() >= 0.01: # This probably helps less than shifting since test data has no noise like this\n",
    "        x = add_noise(x)\n",
    "    if np.random.rand() >= 100: # Adds more compute time than it removes\n",
    "        x = rot(x)\n",
    "    return x\n",
    "\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMPY ATTEMPT OOP 94% 25s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing MNIST Data\n",
      "Setup\n",
      "Running 1 epochs this may take a minute\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (32,784) and (1,784) not aligned: 784 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 101>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x, y \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(X, Y):\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;66;03m# TODO Update to generic like batch == 1 above\u001b[39;00m\n\u001b[1;32m    138\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m784\u001b[39m)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m255\u001b[39m\n\u001b[0;32m--> 139\u001b[0m     guess, loss, correcti \u001b[38;5;241m=\u001b[39m \u001b[43mfor_back_pass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbackpass\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbackpass\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m     dx_out_l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mdx\n\u001b[1;32m    141\u001b[0m     dx_w0_l \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m D0\u001b[38;5;241m.\u001b[39mdx\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mfor_back_pass\u001b[0;34m(x, y, backpass)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_layers):\n\u001b[1;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m---> 58\u001b[0m         \u001b[43mmodel\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     60\u001b[0m         model[i]\u001b[38;5;241m.\u001b[39mforward(model[i\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39moutput)\n",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36mDense.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 15\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweights\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmaximum(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (32,784) and (1,784) not aligned: 784 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "start = time.time()\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "batch = 2\n",
    "\n",
    "class Dense():\n",
    "    '''\n",
    "    Dense Layer\n",
    "    '''\n",
    "    def __init__(self, rows, cols):\n",
    "        self.weights = np.random.randn(rows, cols)*np.sqrt(1/(rows+cols)) # Xavier Initialization\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.res = np.dot(self.weights, x)\n",
    "        self.output = np.maximum(self.res, 0)\n",
    "    \n",
    "    def backward(self, error, x):\n",
    "        self.dx = np.outer(error, x)\n",
    "        self.passing_error = np.dot(self.weights.T, error) * (x > 0)\n",
    "\n",
    "# Data\n",
    "print('Importing MNIST Data')\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train #.reshape(-1, 784)/255\n",
    "X_test = X_test.reshape(-1, 784)/255\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "# Layers\n",
    "print('Setup')\n",
    "D0 = Dense(32, 784)\n",
    "D1 = Dense(32, 32)\n",
    "D2 = Dense(32, 32)\n",
    "out = Dense(10, 32)\n",
    "model = [D0, D1, D2, out]\n",
    "num_layers = len(model)\n",
    "\n",
    "def shuffl3(x, y):\n",
    "    '''\n",
    "    Shuffle the order of incoming images\n",
    "    '''\n",
    "    assert len(x) == len(y)\n",
    "    ids = numpy.random.permutation(len(x))\n",
    "    return x[ids], y[ids]\n",
    "\n",
    "def for_back_pass(x, y, backpass=True):\n",
    "    '''\n",
    "    x is the incoming singular image\n",
    "    y is the label such as [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    backpass is True by default. Set as true if you want to correct weights. False if you want to leave weights alone.\n",
    "    '''\n",
    "    # Forward pass\n",
    "    forward_start = time.time()\n",
    "    for i in range(num_layers):\n",
    "        if i == 0:\n",
    "            model[i].forward(x)\n",
    "        else:\n",
    "            model[i].forward(model[i-1].output)\n",
    "    #https://www.youtube.com/watch?v=mlaLLQofmR8 softmax video\n",
    "    guess = np.exp(model[-1].output - model[-1].output.max()) / np.sum(np.exp(model[-1].output - model[-1].output.max()), axis=0) # Softmax eqn I found somewhere\n",
    "    loss = abs((guess - y)).mean(axis=0)\n",
    "    correct = (np.argmax(y) == np.argmax(guess))\n",
    "    error = (guess - y)\n",
    "    \n",
    "    # Backward Prop\n",
    "    if backpass:\n",
    "        dd = guess*(1-guess)\n",
    "        error = error * dd\n",
    "        for i in range(num_layers):\n",
    "            if i == 0:\n",
    "                model[-1].backward(error, model[-2].output)\n",
    "            elif i == num_layers-1:\n",
    "                model[0].backward(model[1].passing_error, x)\n",
    "            else:\n",
    "                model[num_layers-i-1].backward(model[num_layers-i].passing_error, model[num_layers-i-2].output)\n",
    "    else:\n",
    "        for layer in model:\n",
    "            layer.dx = 0\n",
    "    \n",
    "    return guess, loss, correct\n",
    "\n",
    "def adjust_learning_rate(epoch, lr):\n",
    "    if epoch == 5:\n",
    "        lr = lr / 2\n",
    "    if epoch == 8:\n",
    "        lr = lr / 2\n",
    "    if epoch == 10:\n",
    "        lr = lr / 2\n",
    "    return lr\n",
    "\n",
    "# Loop\n",
    "loss_list = []\n",
    "print('Running {} epochs this may take a minute'.format(epochs))\n",
    "old_dxs = []\n",
    "for i in range(num_layers):\n",
    "    old_dxs.append([0,0])\n",
    "backpass = True\n",
    "validate = True\n",
    "for epoch in range(epochs):\n",
    "    lr = adjust_learning_rate(epoch, lr)\n",
    "    temp_loss = []\n",
    "    correct = []\n",
    "    solver = 'my_momentum_v2'\n",
    "    if batch == 1:\n",
    "        X = X_train\n",
    "        Y = Y_train\n",
    "        X, Y = shuffl3(X, Y)\n",
    "        for x, y in zip(X, Y):\n",
    "            #x = data_aug(x) # This doesn't help much for accuracy 95-97% and it slows things down a bit. Use >97%\n",
    "            x = x.reshape(-1, 784)/255 # It will be faster to do this before the epochs but data_aug easier with square img\n",
    "            x = x[0]\n",
    "            guess, loss, correcti = for_back_pass(x, y, backpass=backpass)\n",
    "            if backpass:\n",
    "                if solver == 'my_momentum_v2':\n",
    "                    # Trying Momentum\n",
    "                    for i, layer in enumerate(model):\n",
    "                        layer.weights = layer.weights - lr*layer.dx - 0.5*lr*old_dxs[i][0] - 0.25*lr*old_dxs[i][1]\n",
    "                        old_dxs[i][1] = old_dxs[i][0]\n",
    "                        old_dxs[i][0] = layer.dx\n",
    "                elif solver == 'adam':\n",
    "                    pass\n",
    "\n",
    "            correct.append(correcti)\n",
    "            \n",
    "    else: # batching will require more epochs\n",
    "        ids = [randint(0, X_train.shape[0]) for i in range(batch)]\n",
    "        X = X_train[ids]\n",
    "        Y = Y_train[ids]\n",
    "        dx_out_l = np.zeros_like(out.weights)\n",
    "        dx_w0_l = np.zeros_like(D0.weights)\n",
    "        dx_w1_l = np.zeros_like(D1.weights)\n",
    "        loss_l = []\n",
    "        correcti_l = []\n",
    "        for x, y in zip(X, Y):\n",
    "            # TODO Update to generic like batch == 1 above\n",
    "            x = x.reshape(-1, 784)/255\n",
    "            guess, loss, correcti = for_back_pass(x, y, backpass=backpass)\n",
    "            dx_out_l += out.dx\n",
    "            dx_w0_l += D0.dx\n",
    "            dx_w1_l += D1.dx\n",
    "            loss_l.append(loss)\n",
    "            correcti_l.append(correcti)\n",
    "        dx_out = dx_out_l / batch\n",
    "        dx_w0 = dx_w0_l / batch\n",
    "        dx_w1 = dx_w1_l / batch\n",
    "        loss = sum(loss_l) / batch\n",
    "        correcti = sum(correcti_l) / batch\n",
    "        if backpass:\n",
    "            if solver == 'my_momentum_v2':\n",
    "                out = out - lr*dx_out - 0.5*lr*old_dx_out - 0.25*lr*vold_dx_out\n",
    "                w0 = w0 - lr*dx_w0 - 0.5*lr*old_dx_w0 - 0.25*lr*vold_dx_w0\n",
    "                w1 = w1 - lr*dx_w1 - 0.5*lr*old_dx_w1 - 0.25*lr*vold_dx_w1\n",
    "                # Trying Momentum\n",
    "                vold_dx_out = old_dx_out\n",
    "                vold_dx_w0 = old_dx_w0\n",
    "                vold_dx_w1 = old_dx_w1\n",
    "                old_dx_out = dx_out\n",
    "                old_dx_w0 = dx_w0\n",
    "                old_dx_w1 = dx_w1\n",
    "            elif solver == 'adam':\n",
    "                pass\n",
    "\n",
    "        correct.append(correcti)\n",
    "        \n",
    "    correct_percent = sum(correct) / len(correct)\n",
    "    loss_list.append(loss)\n",
    "    if epochs > 10000:\n",
    "        if epoch % 10000 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs > 1000:\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs > 100:\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs < 100:\n",
    "        print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "print('Final Epoch Result')\n",
    "print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "        \n",
    "if validate:\n",
    "    print()\n",
    "    print('Validating...')\n",
    "    X = X_test\n",
    "    Y = Y_test\n",
    "    X, Y = shuffl3(X, Y)\n",
    "    correct_l = []\n",
    "    for x, y in zip(X, Y):\n",
    "        guess, loss, correcti = for_back_pass(x, y, backpass=False)\n",
    "        correct_l.append(correcti)\n",
    "    correct_percent = sum(correct_l) / len(correct_l)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print('######################################')\n",
    "    print('VALIDATION CORRECT = {}'.format(correct_percent))\n",
    "    print('######################################')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMPY ATTEMPT 1 90% 45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing MNIST Data\n",
      "Setup\n",
      "Running 1 epochs this may take a minute\n",
      "Epoch0 Time = 46.37789297103882s loss=0.0023756201475579456 accuracy = 0.8818166666666667\n",
      "Final Epoch Result\n",
      "Epoch0 Time = 46.37827777862549s loss=0.0023756201475579456 accuracy = 0.8818166666666667\n",
      "\n",
      "Validating...\n",
      "\n",
      "\n",
      "\n",
      "######################################\n",
      "VALIDATION CORRECT = 0.9448\n",
      "######################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "start = time.time()\n",
    "epochs = 1\n",
    "lr = 0.01\n",
    "batch = 1\n",
    "\n",
    "# Data\n",
    "print('Importing MNIST Data')\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train #.reshape(-1, 784)/255\n",
    "X_test = X_test.reshape(-1, 784)/255\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "# Layers\n",
    "print('Setup')\n",
    "w0 = np.random.randn(64, 784)*np.sqrt(1/(64+784)) # Xavier Initialization\n",
    "w1 = np.random.randn(32, 64)*np.sqrt(1/(32+64))\n",
    "out = np.random.randn(10, 32)*np.sqrt(1/(10+32))\n",
    "\n",
    "def shuffl3(x, y):\n",
    "    '''\n",
    "    Shuffle the order of incoming images\n",
    "    '''\n",
    "    assert len(x) == len(y)\n",
    "    ids = numpy.random.permutation(len(x))\n",
    "    return x[ids], y[ids]\n",
    "\n",
    "def for_back_pass(x, y, backpass=True):\n",
    "    '''\n",
    "    x is the incoming singular image\n",
    "    y is the label such as [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    backpass is True by default. Set as true if you want to correct weights. False if you want to leave weights alone.\n",
    "    '''\n",
    "    # Forward pass\n",
    "    forward_start = time.time()\n",
    "    res_w0 = np.dot(w0, x)\n",
    "    res_rel0 = np.maximum(res_w0, 0)\n",
    "    res_w1 = np.dot(w1, res_rel0)\n",
    "    res_rel1 = np.maximum(res_w1, 0)\n",
    "    res_out = np.dot(out, res_rel1)\n",
    "    #https://www.youtube.com/watch?v=mlaLLQofmR8 softmax video\n",
    "    guess = np.exp(res_out - res_out.max()) / np.sum(np.exp(res_out - res_out.max()), axis=0) # Softmax eqn I found somewhere\n",
    "    loss = abs((guess - y)).mean(axis=0)\n",
    "    correct = (np.argmax(y) == np.argmax(guess))\n",
    "    error = (guess - y)\n",
    "    \n",
    "    # Backward Prop\n",
    "    if backpass:\n",
    "        dd = guess*(1-guess)\n",
    "        error = error * dd\n",
    "        dx_out = np.outer(error, res_rel1)\n",
    "        error = np.dot(out.T, error) * (res_rel1 > 0)\n",
    "        dx_w1 = np.outer(error, res_rel0)\n",
    "        error = np.dot(w1.T, error) * (res_rel0 > 0)\n",
    "        dx_w0 = np.outer(error, x)\n",
    "    else:\n",
    "        dx_out, dx_w0, dx_w1 = 0, 0, 0\n",
    "    \n",
    "    return dx_out, dx_w0, dx_w1, guess, loss, correct\n",
    "\n",
    "# Loop\n",
    "loss_list = []\n",
    "print('Running {} epochs this may take a minute'.format(epochs))\n",
    "vold_dx_out = 0\n",
    "vold_dx_w0 = 0\n",
    "vold_dx_w1 = 0\n",
    "old_dx_out = 0\n",
    "old_dx_w0 = 0\n",
    "old_dx_w1 = 0\n",
    "backpass = True\n",
    "validate = True\n",
    "for epoch in range(epochs):\n",
    "    if epoch == 5:\n",
    "        lr = lr / 2\n",
    "    if epoch == 8:\n",
    "        lr = lr / 2\n",
    "    if epoch == 10:\n",
    "        lr = lr / 2\n",
    "    temp_loss = []\n",
    "    correct = []\n",
    "    solver = 'my_momentum_v2'\n",
    "    if batch == 1:\n",
    "        X = X_train\n",
    "        Y = Y_train\n",
    "        X, Y = shuffl3(X, Y)\n",
    "        for x, y in zip(X, Y):\n",
    "            #x = data_aug(x) # This doesn't help much for accuracy 95-97% and it slows things down a bit. Use >97%\n",
    "            x = x.reshape(-1, 784)/255 # It will be faster to do this before the epochs but data_aug easier with square img\n",
    "            x = x[0]\n",
    "            dx_out, dx_w0, dx_w1, guess, loss, correcti = for_back_pass(x, y, backpass=backpass)\n",
    "            if backpass:\n",
    "                if solver == 'my_momentum_v2':\n",
    "                    out = out - lr*dx_out - 0.5*lr*old_dx_out - 0.25*lr*vold_dx_out\n",
    "                    w0 = w0 - lr*dx_w0 - 0.5*lr*old_dx_w0 - 0.25*lr*vold_dx_w0\n",
    "                    w1 = w1 - lr*dx_w1 - 0.5*lr*old_dx_w1 - 0.25*lr*vold_dx_w1\n",
    "                    # Trying Momentum\n",
    "                    vold_dx_out = old_dx_out\n",
    "                    vold_dx_w0 = old_dx_w0\n",
    "                    vold_dx_w1 = old_dx_w1\n",
    "                    old_dx_out = dx_out\n",
    "                    old_dx_w0 = dx_w0\n",
    "                    old_dx_w1 = dx_w1\n",
    "                elif solver == 'adam':\n",
    "                    pass\n",
    "\n",
    "            correct.append(correcti)\n",
    "            \n",
    "    else: # batching will require more epochs\n",
    "        ids = [randint(0, X_train.shape[0]) for i in range(batch)]\n",
    "        X = X_train[ids]\n",
    "        Y = Y_train[ids]\n",
    "        dx_out_l = np.zeros_like(out)\n",
    "        dx_w0_l = np.zeros_like(w0)\n",
    "        dx_w1_l = np.zeros_like(w1)\n",
    "        loss_l = []\n",
    "        correcti_l = []\n",
    "        for x, y in zip(X, Y):\n",
    "            dx_out, dx_w0, dx_w1, guess, loss, correcti = for_back_pass(x, y, backpass=backpass)\n",
    "            dx_out_l += dx_out\n",
    "            dx_w0_l += dx_w0\n",
    "            dx_w1_l += dx_w1\n",
    "            loss_l.append(loss)\n",
    "            correcti_l.append(correcti)\n",
    "        dx_out = dx_out_l / batch\n",
    "        dx_w0 = dx_w0_l /batch\n",
    "        dx_w1 = dx_w1_l /batch\n",
    "        loss = sum(loss_l)/batch\n",
    "        correcti = sum(correcti_l)/batch\n",
    "        if backpass:\n",
    "            if solver == 'my_momentum_v2':\n",
    "                out = out - lr*dx_out - 0.5*lr*old_dx_out - 0.25*lr*vold_dx_out\n",
    "                w0 = w0 - lr*dx_w0 - 0.5*lr*old_dx_w0 - 0.25*lr*vold_dx_w0\n",
    "                w1 = w1 - lr*dx_w1 - 0.5*lr*old_dx_w1 - 0.25*lr*vold_dx_w1\n",
    "                # Trying Momentum\n",
    "                vold_dx_out = old_dx_out\n",
    "                vold_dx_w0 = old_dx_w0\n",
    "                vold_dx_w1 = old_dx_w1\n",
    "                old_dx_out = dx_out\n",
    "                old_dx_w0 = dx_w0\n",
    "                old_dx_w1 = dx_w1\n",
    "            elif solver == 'adam':\n",
    "                pass\n",
    "\n",
    "        correct.append(correcti)\n",
    "        \n",
    "    correct_percent = sum(correct) / len(correct)\n",
    "    loss_list.append(loss)\n",
    "    if epochs > 10000:\n",
    "        if epoch % 10000 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs > 1000:\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs > 100:\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs < 100:\n",
    "        print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "print('Final Epoch Result')\n",
    "print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "        \n",
    "if validate:\n",
    "    print()\n",
    "    print('Validating...')\n",
    "    X = X_test\n",
    "    Y = Y_test\n",
    "    X, Y = shuffl3(X, Y)\n",
    "    correct_l = []\n",
    "    for x, y in zip(X, Y):\n",
    "        dx_out, dx_w0, dx_w1, guess, loss, correcti = for_back_pass(x, y, backpass=False)\n",
    "        correct_l.append(correcti)\n",
    "    correct_percent = sum(correct_l) / len(correct_l)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print('######################################')\n",
    "    print('VALIDATION CORRECT = {}'.format(correct_percent))\n",
    "    print('######################################')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Spot to Validate if Model Interrupted\n",
    "98% @ 5min 20 epochs 794-64-32-10 Data Aug lr decay\n",
    "98.4% @ 37min 20 epochs 794-128-64-10 Data Aug lr decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Validating...\n",
      "\n",
      "\n",
      "\n",
      "######################################\n",
      "VALIDATION CORRECT = 0.9806\n",
      "######################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "if validate:\n",
    "    print()\n",
    "    print('Validating...')\n",
    "    X = X_test\n",
    "    Y = Y_test\n",
    "    X, Y = shuffl3(X, Y)\n",
    "    correct_l = []\n",
    "    for x, y in zip(X, Y):\n",
    "        dx_out, dx_w0, dx_w1, guess, loss, correcti = for_back_pass(x, y, backpass=False)\n",
    "        correct_l.append(correcti)\n",
    "    correct_percent = sum(correct_l) / len(correct_l)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print('######################################')\n",
    "    print('VALIDATION CORRECT = {}'.format(correct_percent))\n",
    "    print('######################################')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area Getting Gradients Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual = [[0 1 0 0 0]]\n",
      "res_out = [0.   0.8  0.5  0.25 0.7 ]\n",
      "guess = [0.12236846 0.27233602 0.20175148 0.15712421 0.24641982]\n",
      "error = [[-0.12236846  0.72766398 -0.20175148 -0.15712421 -0.24641982]]\n",
      "dx_guess = [0.10739442 0.19816911 0.16104782 0.1324362  0.18569709]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "actual = np.array([[0, 1, 0, 0, 0]])\n",
    "print('actual = {}'.format(actual))\n",
    "res_out = np.array([[0, .8, .5, .25, .7]])\n",
    "res_out = res_out[0]\n",
    "print('res_out = {}'.format(res_out))\n",
    "#guess = 1/(1+np.exp(-res_out))\n",
    "guess = np.exp(res_out - res_out.max()) / np.sum(np.exp(res_out - res_out.max()), axis=0)\n",
    "print('guess = {}'.format(guess))\n",
    "error = (actual-guess)\n",
    "print('error = {}'.format(error))\n",
    "dx_guess = guess*(1-guess)\n",
    "print('dx_guess = {}'.format(dx_guess))\n",
    "\n",
    "print((np.argmax(actual) == np.argmax(guess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
