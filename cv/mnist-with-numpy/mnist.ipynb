{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import gzip\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import time\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Keras 92% 16s For Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "60000/60000 [==============================] - 16s 269us/step - loss: 0.3737 - accuracy: 0.8960\n",
      "10000/10000 [==============================] - 1s 89us/step\n",
      "test loss, test acc: [0.28741244918704034, 0.921999990940094]\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Convolution2D, MaxPooling2D  \n",
    "from keras.models import Sequential\n",
    "\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = np.expand_dims(X_train, axis=3)\n",
    "X_test = np.expand_dims(X_test, axis=3)\n",
    "X_train = keras.utils.normalize(X_train, axis=1)\n",
    "X_test = keras.utils.normalize(X_test, axis=1)\n",
    "Y_train = keras.utils.to_categorical(Y_train)\n",
    "Y_test = keras.utils.to_categorical(Y_test)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(28, name='dense_in', activation='relu', input_shape=(28,28,1)))\n",
    "model.add(Flatten(name='flat'))\n",
    "model.add(Dense(10, name='dense_last', activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "model.fit(X_train, Y_train, batch_size=32, epochs=1, verbose=1)\n",
    "results = model.evaluate(X_test, Y_test, batch_size=32)\n",
    "print('test loss, test acc:', results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues\n",
    "- Try Cross Entropy Loss\n",
    "- Find why algo so slow\n",
    "- ***Working*** - Batching\n",
    "- Dropout\n",
    "- Max Pooling\n",
    "- Each pass is fast, but algo still slow, batch might help"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NUMPY ATTEMPT 90% 45s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing MNIST Data\n",
      "Setup\n",
      "Running 10000 epochs\n",
      "Epoch0 Time = 0.5733880996704102s loss=0.17997360085112396 accuracy = 0.0625\n",
      "Epoch1000 Time = 4.913451433181763s loss=0.17586762949243145 accuracy = 0.25\n",
      "Epoch2000 Time = 9.274529457092285s loss=0.1192390267613016 accuracy = 0.625\n",
      "Epoch3000 Time = 13.661634922027588s loss=0.08462221409664564 accuracy = 0.6875\n",
      "Epoch4000 Time = 18.089749574661255s loss=0.08985055064347795 accuracy = 0.6875\n",
      "Epoch5000 Time = 22.486852407455444s loss=0.06234810262591403 accuracy = 0.875\n",
      "Epoch6000 Time = 26.902969360351562s loss=0.07767183326828908 accuracy = 0.75\n",
      "Epoch7000 Time = 31.22402787208557s loss=0.03166464271126266 accuracy = 0.9375\n",
      "Epoch8000 Time = 35.65115547180176s loss=0.05187182093977273 accuracy = 0.8125\n",
      "Epoch9000 Time = 40.06325602531433s loss=0.04865865544904874 accuracy = 0.875\n",
      "Final Epoch Result\n",
      "Epoch9999 Time = 44.80560278892517s loss=0.0185971299321406 accuracy = 0.9375\n",
      "\n",
      "Validating...\n",
      "\n",
      "\n",
      "\n",
      "######################################\n",
      "VALIDATION CORRECT = 0.9035\n",
      "######################################\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Setup\n",
    "start = time.time()\n",
    "epochs = 10000\n",
    "lr = 0.01\n",
    "batch = 16\n",
    "\n",
    "# Data\n",
    "print('Importing MNIST Data')\n",
    "(X_train, Y_train), (X_test, Y_test) = mnist.load_data()\n",
    "X_train = X_train.reshape(-1, 784)/255\n",
    "X_test = X_test.reshape(-1, 784)/255\n",
    "from keras.utils.np_utils import to_categorical\n",
    "Y_train = to_categorical(Y_train)\n",
    "Y_test = to_categorical(Y_test)\n",
    "\n",
    "# Layers\n",
    "print('Setup')\n",
    "w0 = np.random.randn(64, 784)*np.sqrt(1/(64+784)) # Xavier Initialization\n",
    "w1 = np.random.randn(32, 64)*np.sqrt(1/(32+64))\n",
    "out = np.random.randn(10, 32)*np.sqrt(1/(10+32))\n",
    "\n",
    "def shuffl3(x, y):\n",
    "    '''\n",
    "    Shuffle the order of incoming images\n",
    "    '''\n",
    "    assert len(x) == len(y)\n",
    "    ids = numpy.random.permutation(len(x))\n",
    "    return x[ids], y[ids]\n",
    "\n",
    "def for_back_pass(x, y, backpass=True):\n",
    "    '''\n",
    "    x is the incoming singular image\n",
    "    y is the label such as [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
    "    backpass is True by default. Set as true if you want to correct weights. False if you want to leave weights alone.\n",
    "    '''\n",
    "    # Forward pass\n",
    "    forward_start = time.time()\n",
    "    res_w0 = np.dot(w0, x)\n",
    "    res_rel0 = np.maximum(res_w0, 0)\n",
    "    res_w1 = np.dot(w1, res_rel0)\n",
    "    res_rel1 = np.maximum(res_w1, 0)\n",
    "    res_out = np.dot(out, res_rel1)\n",
    "    #https://www.youtube.com/watch?v=mlaLLQofmR8 softmax video\n",
    "    guess = np.exp(res_out - res_out.max()) / np.sum(np.exp(res_out - res_out.max()), axis=0) # Softmax eqn I found somewhere\n",
    "    loss = abs((guess - y)).mean(axis=0)\n",
    "    correct = (np.argmax(y) == np.argmax(guess))\n",
    "    error = (guess - y)\n",
    "    \n",
    "    # Backward Prop\n",
    "    if backpass:\n",
    "        dd = guess*(1-guess)\n",
    "        error = error * dd\n",
    "        dx_out = np.outer(error, res_rel1)\n",
    "        error = np.dot(out.T, error) * (res_rel1 > 0)\n",
    "        dx_w1 = np.outer(error, res_rel0)\n",
    "        error = np.dot(w1.T, error) * (res_rel0 > 0)\n",
    "        dx_w0 = np.outer(error, x)\n",
    "    else:\n",
    "        dx_out, dx_w0, dx_w1 = 0, 0, 0\n",
    "    \n",
    "    return dx_out, dx_w0, dx_w1, guess, loss, correct\n",
    "\n",
    "# Loop\n",
    "loss_list = []\n",
    "print('Running {} epochs'.format(epochs))\n",
    "vold_dx_out = 0\n",
    "vold_dx_w0 = 0\n",
    "vold_dx_w1 = 0\n",
    "old_dx_out = 0\n",
    "old_dx_w0 = 0\n",
    "old_dx_w1 = 0\n",
    "backpass = True\n",
    "validate = True\n",
    "for epoch in range(epochs):\n",
    "    temp_loss = []\n",
    "    correct = []\n",
    "    solver = 'my_momentum_v2'\n",
    "    if batch == 1:\n",
    "        X = X_train\n",
    "        Y = Y_train\n",
    "        X, Y = shuffl3(X, Y)\n",
    "        for x, y in zip(X, Y):\n",
    "            dx_out, dx_w0, dx_w1, guess, loss, correcti = for_back_pass(x, y, backpass=backpass)\n",
    "            if backpass:\n",
    "                if solver == 'my_momentum_v2':\n",
    "                    out = out - lr*dx_out - 0.5*lr*old_dx_out - 0.25*lr*vold_dx_out\n",
    "                    w0 = w0 - lr*dx_w0 - 0.5*lr*old_dx_w0 - 0.25*lr*vold_dx_w0\n",
    "                    w1 = w1 - lr*dx_w1 - 0.5*lr*old_dx_w1 - 0.25*lr*vold_dx_w1\n",
    "                    # Trying Momentum\n",
    "                    vold_dx_out = old_dx_out\n",
    "                    vold_dx_w0 = old_dx_w0\n",
    "                    vold_dx_w1 = old_dx_w1\n",
    "                    old_dx_out = dx_out\n",
    "                    old_dx_w0 = dx_w0\n",
    "                    old_dx_w1 = dx_w1\n",
    "                elif solver == 'adam':\n",
    "                    pass\n",
    "\n",
    "            correct.append(correcti)\n",
    "            \n",
    "    else: # batching will require more epochs\n",
    "        ids = [randint(0, X_train.shape[0]) for i in range(batch)]\n",
    "        X = X_train[ids]\n",
    "        Y = Y_train[ids]\n",
    "        dx_out_l = np.zeros_like(out)\n",
    "        dx_w0_l = np.zeros_like(w0)\n",
    "        dx_w1_l = np.zeros_like(w1)\n",
    "        loss_l = []\n",
    "        correcti_l = []\n",
    "        for x, y in zip(X, Y):\n",
    "            dx_out, dx_w0, dx_w1, guess, loss, correcti = for_back_pass(x, y, backpass=backpass)\n",
    "            dx_out_l += dx_out\n",
    "            dx_w0_l += dx_w0\n",
    "            dx_w1_l += dx_w1\n",
    "            loss_l.append(loss)\n",
    "            correcti_l.append(correcti)\n",
    "        dx_out = dx_out_l / batch\n",
    "        dx_w0 = dx_w0_l /batch\n",
    "        dx_w1 = dx_w1_l /batch\n",
    "        loss = sum(loss_l)/batch\n",
    "        correcti = sum(correcti_l)/batch\n",
    "        if backpass:\n",
    "            if solver == 'my_momentum_v2':\n",
    "                out = out - lr*dx_out - 0.5*lr*old_dx_out - 0.25*lr*vold_dx_out\n",
    "                w0 = w0 - lr*dx_w0 - 0.5*lr*old_dx_w0 - 0.25*lr*vold_dx_w0\n",
    "                w1 = w1 - lr*dx_w1 - 0.5*lr*old_dx_w1 - 0.25*lr*vold_dx_w1\n",
    "                # Trying Momentum\n",
    "                vold_dx_out = old_dx_out\n",
    "                vold_dx_w0 = old_dx_w0\n",
    "                vold_dx_w1 = old_dx_w1\n",
    "                old_dx_out = dx_out\n",
    "                old_dx_w0 = dx_w0\n",
    "                old_dx_w1 = dx_w1\n",
    "            elif solver == 'adam':\n",
    "                pass\n",
    "\n",
    "        correct.append(correcti)\n",
    "        \n",
    "    correct_percent = sum(correct) / len(correct)\n",
    "    loss_list.append(loss)\n",
    "    if epochs > 10000:\n",
    "        if epoch % 10000 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs > 1000:\n",
    "        if epoch % 1000 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs > 100:\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "    elif epochs < 100:\n",
    "        print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "print('Final Epoch Result')\n",
    "print('Epoch{} Time = {}s loss={} accuracy = {}'.format(epoch, time.time() - start, loss, correct_percent))\n",
    "        \n",
    "if validate:\n",
    "    print()\n",
    "    print('Validating...')\n",
    "    X = X_test\n",
    "    Y = Y_test\n",
    "    X, Y = shuffl3(X, Y)\n",
    "    correct_l = []\n",
    "    for x, y in zip(X, Y):\n",
    "        dx_out, dx_w0, dx_w1, guess, loss, correcti = for_back_pass(x, y, backpass=False)\n",
    "        correct_l.append(correcti)\n",
    "    correct_percent = sum(correct_l) / len(correct_l)\n",
    "    print()\n",
    "    print()\n",
    "    print()\n",
    "    print('######################################')\n",
    "    print('VALIDATION CORRECT = {}'.format(correct_percent))\n",
    "    print('######################################')\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Area Getting Gradients Working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual = [[0 1 0 0 0]]\n",
      "res_out = [0.   0.8  0.5  0.25 0.7 ]\n",
      "guess = [0.12236846 0.27233602 0.20175148 0.15712421 0.24641982]\n",
      "error = [[-0.12236846  0.72766398 -0.20175148 -0.15712421 -0.24641982]]\n",
      "dx_guess = [0.10739442 0.19816911 0.16104782 0.1324362  0.18569709]\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "actual = np.array([[0, 1, 0, 0, 0]])\n",
    "print('actual = {}'.format(actual))\n",
    "res_out = np.array([[0, .8, .5, .25, .7]])\n",
    "res_out = res_out[0]\n",
    "print('res_out = {}'.format(res_out))\n",
    "#guess = 1/(1+np.exp(-res_out))\n",
    "guess = np.exp(res_out - res_out.max()) / np.sum(np.exp(res_out - res_out.max()), axis=0)\n",
    "print('guess = {}'.format(guess))\n",
    "error = (actual-guess)\n",
    "print('error = {}'.format(error))\n",
    "dx_guess = guess*(1-guess)\n",
    "print('dx_guess = {}'.format(dx_guess))\n",
    "\n",
    "print((np.argmax(actual) == np.argmax(guess)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
